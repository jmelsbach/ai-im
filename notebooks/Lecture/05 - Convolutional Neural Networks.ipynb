{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conceptual-driver",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johannesmelsbach/ai-im/blob/main/notebooks/Lecture/05%20-%20Convolutional%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-rabbit",
   "metadata": {},
   "source": [
    "# Lecture 05 Notebook - Convolutional Neural Networks\n",
    "\n",
    "The notebook is accomponying Lecture 05 - Introduction to Convolutional Neural Networks.\n",
    "\n",
    "This and the following notebooks accompany the lecture to show you how to implement neural networks and the like using PyTorch & Co with relatively little effort. You will see that most of the concepts, techniques and functions are already available and it is often very easy to make use of them. All notebooks will be connected to [Colab](https://colab.research.google.com/notebooks/intro.ipynb) such that you can directly execute the code and play with it by yourself. We encourage you to not just execute the code but to also think about it in detail. \n",
    "\n",
    "Further, we implemented the code such that each subchapter is executable without the need of executing previous subchapter. Imports and helper functions might appear to be redundant.\n",
    "\n",
    "This notebook introduces Computer Vision and Convolutional Neural Networks (CNNs). We'll first show different color spaces. Next, we'll implement different building blocks (i.e., Convolutional Operation, Padding and Stride, Nonlinearity, Pooling, Channels). Finally, we will develop a small CNN using PyTorch.\n",
    "\n",
    "Have fun & keep coding!\n",
    "\n",
    "Authors: Johannes Melsbach & Jannik Rößler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-nicaragua",
   "metadata": {},
   "source": [
    "## 1. Color Spaces\n",
    "\n",
    "First, let's have a look at how the computer sees images, and what different color spaces look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-understanding",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-coordinator",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, figsize=(4, 4)):\n",
    "    \"\"\"\n",
    "    Plot an image\n",
    "    \n",
    "    :param img: Image which should be plotted\n",
    "    :param figsize: Size of the figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_from_url(url):\n",
    "    \"\"\"\n",
    "    Reading images from url\n",
    "    \n",
    "    :param url: URL\n",
    "    :return raw image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-realtor",
   "metadata": {},
   "source": [
    "### 1.1 Single Channel\n",
    "\n",
    "Images with just one channel such as 1-bit monochrome or 8-bit grayscale.\n",
    "\n",
    "*Recap*: A channel is a single basic color in an image.\n",
    "\n",
    "#### 1.1.1 Binary image / 1-bit-monochrome\n",
    "\n",
    "* Pixel values are either 0 (black) or 1 (white). No values in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGR is the default color space when reading images with cv2\n",
    "img_raw = read_image_from_url(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/one.png\")\n",
    "img_gray = cv2.cvtColor(img_raw, cv2.COLOR_BGR2GRAY)  # Cast into 8-bit grayscale image\n",
    "img_monochrome = (img_gray/255).astype('uint8')  # Turn into 1-bit monochrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(img_monochrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-restoration",
   "metadata": {},
   "source": [
    "#### 1.1.2 8-bit-grayscale\n",
    "\n",
    "* Pixel values are between 0 (black) and 255 (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.resize(img_gray, (25,25))  # Resize to 25x25 (for illustration purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-spanking",
   "metadata": {},
   "source": [
    "Let's plot the image with the values for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(img_gray)\n",
    "df = df.iloc[1:]  # For some reasons we have to drop the first row. Otherwise the colormap does not work properly!\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('gray', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-insert",
   "metadata": {},
   "source": [
    "### 1.2 Multiple Channels\n",
    "\n",
    "Images with multiple channels such as RGB (red, gree, blue) with three channels and HSV (hue, saturatin, value) with three channels.\n",
    "\n",
    "#### 1.2.1 BGR Color Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGR is the default color space when reading images with cv2\n",
    "img_raw = read_image_from_url(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/happy_child.jpg\")\n",
    "img_rgb = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)  # Cast into RGB image\n",
    "img_hsv = cv2.cvtColor(img_raw, cv2.COLOR_BGR2HSV)  # Cast into HSV image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(img_raw, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-detroit",
   "metadata": {},
   "source": [
    "#### 1.2.2 RGB Color Space\n",
    "\n",
    "RGB color space is based on the theory that all visible colors can be created using red, green, and blue. Each pixel has three values (called RGB value), where each value is between 0 (no color) and 255 (full saturation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(img_rgb, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-norwegian",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Red channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = img_rgb.copy()\n",
    "# set green and red channels to 0\n",
    "red[:, :, 1] = 0  # Set green channel to 0\n",
    "red[:, :, 2] = 0  # Set blue channel to 0\n",
    "plot_image(red, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-joyce",
   "metadata": {},
   "source": [
    "#### 1.2.2.2 Green channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = img_rgb.copy()\n",
    "# set green and red channels to 0\n",
    "green[:, :, 0] = 0  # Set red channel to 0\n",
    "green[:, :, 2] = 0  # Set blue channel to 0\n",
    "plot_image(green, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-permission",
   "metadata": {},
   "source": [
    "#### 1.2.2.3 Blue channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = img_rgb.copy()\n",
    "# set green and red channels to 0\n",
    "blue[:, :, 0] = 0  # Set red channel to 0\n",
    "blue[:, :, 1] = 0  # Set green channel to 0\n",
    "plot_image(blue, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-ensemble",
   "metadata": {},
   "source": [
    "#### 1.2.3 HSV Color Space\n",
    "\n",
    "Each pixel has three values: hue, saturation and value (HSV)\n",
    "\n",
    "* Hue: Hue is the color portion of the model, expressed as a number from 0 to 360 degrees. For example, red falls between 0 and 60 degrees.\n",
    "* Saturation: Saturation describes the amount of gray in a color, from 0 to 100 percent.\n",
    "* Value (or brightness): Value describes the brightness of the color, from 0 to 100 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(img_hsv, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-helmet",
   "metadata": {},
   "source": [
    "## 2. Building Block: Convolutional Operation\n",
    "\n",
    "<a name=\"conv-operation\"></a>\n",
    "\n",
    "*Recap:* \n",
    "\n",
    "* The convolutional operation measures the overlap between two functions, that is (informally) whether a part of the input contains a specific feature. Latter is represented by the kernel\n",
    "\n",
    "*CNN terminology:*\n",
    "* X is called the input (e.g., the image)\n",
    "* W is called the kernel, sometimes called filter\n",
    "* Y is the output, sometimes referred to as feature map or activation map\n",
    "\n",
    "In PyTorch we can implement a convolutional layer which applies the convolutional operation. However, in PyTorch other operations such as nonlinearity (applying activation function) and pooling have to be applied separately using other classes.\n",
    "\n",
    "We'll use the same example from the lecture! (see Slide #17; black bar at the left of the image + vertical edge kernel (going from darker pixels from the left to brighter pixels to the right))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-street",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-enhancement",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, figsize=(4, 4)):\n",
    "    \"\"\"\n",
    "    Plot an image\n",
    "    \n",
    "    :param img: Image which should be plotted\n",
    "    :param figsize: Size of the figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    \"\"\"\n",
    "    Cast an (cv2) image into a tensor with shape (1,1,height,width)\n",
    "    \n",
    "    :param img: Image which should be cast into a tensor\n",
    "    \n",
    "    :return: Tensor\n",
    "    \"\"\"\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_tensor = torch.from_numpy(img_gray)\n",
    "    img_tensor = img_tensor.reshape(1,1,img_tensor.shape[0],img_tensor.shape[1]).float()  #. Reshape image . PyTorch expects 4-dimensional input\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_from_url(url):\n",
    "    \"\"\"\n",
    "    Reading images from url\n",
    "    \n",
    "    :param url: URL\n",
    "    :return raw image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-grenada",
   "metadata": {},
   "source": [
    "### 2.1 Create Image\n",
    "\n",
    "* 1-bit monochrome image (0 is black, 1 is wite)\n",
    "* Vertical black bar at the very left of the image, rest is white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor=torch.zeros(1,1,4,4)  # Shape (number of samples, number of channels, height, width)\n",
    "img_tensor[0,0,:,:] = 1\n",
    "img_tensor[0,0,:,0] = 0\n",
    "print(f\"Image shape: {img_tensor.shape}\")\n",
    "print(f\"Image: \\n{img_tensor} \\n\")\n",
    "plot_image(img_tensor.reshape(img_tensor.shape[2], img_tensor.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-hollow",
   "metadata": {},
   "source": [
    "### 2.2 Create Convolutional Layer\n",
    "\n",
    "In PyTorch we can create a convolutional layer (which applies the convolutional operation) simply by creating a Conv2d object. We only need to provide some parameters such as the number of input and output channels, the kernel size and the size of stride and padding.\n",
    "\n",
    "In our example the parameters are the following:\n",
    "* number of input channels: 1 (Number of channels in the input. Here: 1)\n",
    "* number of output channels: 1 (Number of channels in the output (feature map). Here: 1)\n",
    "* kernel size: 2 (Often we use homogeneous kernels with $w_h=w_w$. Thus we don't have to provide width and height but only a single integer. Here: 2)\n",
    "* stride: 1 (Refers to the number of rows and columns we move the kernel. Here: 1).\n",
    "* padding: 0 (Adding extra pixels around the boundary of the input image. Here: No padding. 0)\n",
    "\n",
    "**Important**: In our example we'll use the kernel from the lecture with no bias. Usually, in PyTorch kernel and bias are randomly initialized and you don't have to modify them! This is just for illustration purposes.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d): Creates a Convolutional Layer which applies the convolutinal operation\n",
    "* [state_dict](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html): Dictionary which contains the parameter of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride = 1, padding = 0)  # Conv2d object\n",
    "\n",
    "\"\"\"\n",
    "In practice you would not do that\n",
    "\n",
    "PyTorch randomly initializes the kernel. In order to illustrate the example from the lecture, we have to set the kernels parameters by ourself.\n",
    "\"\"\"\n",
    "kernel_parameters = torch.tensor([[-1.0, 1.0],[-1.0, 1.0]])  # Define parameter\n",
    "conv.state_dict()['weight'][0][0]=kernel_parameters  # Set kernel\n",
    "conv.state_dict()['bias'][0]=0.0  # Set bias to zero.\n",
    "\"\"\"\n",
    "Until here!\n",
    "\n",
    "In practice you would not do that\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Kernel shape: {conv.state_dict()['weight'].shape}\")\n",
    "print(f\"Kernel: \\n{conv.state_dict()['weight']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-registration",
   "metadata": {},
   "source": [
    "### 2.3 Calculate Feature Map\n",
    "\n",
    "After creating the convolutional layer, we can forward propagate the image through this layer in order to calculate the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map=conv(img_tensor)  # Provide the input to the conv2d object to calculate the feature map\n",
    "print(f\"Feature map shape: {feature_map.shape}\")\n",
    "print(f\"Feature map: \\n{feature_map} \\n\")\n",
    "plot_image(feature_map.detach().numpy().reshape(feature_map.shape[2], feature_map.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-domestic",
   "metadata": {},
   "source": [
    "### 2.4 Vertical Edge Detector in Practice\n",
    "\n",
    "We'll use the convolutional layer from above (vertical edge detector) to illustrate how the convolutional operation measures the overlap between a vertical edge detector and different images.\n",
    "\n",
    "We'll use a function which expects the path to the image as an input. You can call the function with different paths, to see how the feature map changes depending on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vertical_edges(url):\n",
    "    \"\"\"\n",
    "    Applies the convolutional operation with a vertical edge detector and plots the feature map.\n",
    "    \n",
    "    :param url: URL of the image\n",
    "    \"\"\"\n",
    "    img_raw = read_image_from_url(url)\n",
    "    img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = image_to_tensor(img_raw)\n",
    "    \n",
    "    # Plot input\n",
    "    plot_image(img_tensor.detach().numpy().reshape(img_tensor.shape[2],img_tensor.shape[3]))\n",
    "    \n",
    "    # Calculate feature map\n",
    "    feature_map = conv(img_tensor)\n",
    "    print(f\"Feature map shape: {feature_map.shape}\")\n",
    "    \n",
    "    # Plot feature map\n",
    "    plot_image(feature_map.detach().numpy().reshape(feature_map.shape[2],feature_map.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test these images by yourself. Alternatively, use your own images!\n",
    "detect_vertical_edges(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/one.png\")\n",
    "#detect_vertical_edges(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/four.png\")\n",
    "#detect_vertical_edges(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/five.png\")\n",
    "#detect_vertical_edges(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/nine.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-collection",
   "metadata": {},
   "source": [
    "## 3. Building Block: Padding and Stride\n",
    "\n",
    "*Recap:*\n",
    "* Padding: Add extra pixels (typically zero) around the boundary of the input image\n",
    "* Stride: Refers to the number of rows and columns we move the kernel\n",
    "\n",
    "In practice, we rarely use inhomogeneous padding and stride, that is, we usually have $p_h=p_w$ and $s_h=s_w$\n",
    "\n",
    "By default (at least in PyTorch), $p_h=p_w=0$ and $s_h=s_w=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-sunset",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-spiritual",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, figsize=(4, 4)):\n",
    "    \"\"\"\n",
    "    Plot an image\n",
    "    \n",
    "    :param img: Image which should be plotted\n",
    "    :param figsize: Size of the figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-minnesota",
   "metadata": {},
   "source": [
    "### 3.1 Create Image\n",
    "\n",
    "* 1-bit monochrome image (0 is black, 1 is wite)\n",
    "* Vertical black line at the very left of the image, rest is white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor=torch.zeros(1,1,4,4)  # Shape (number of samples, number of channels, height, width)\n",
    "img_tensor[0,0,:,:] = 1\n",
    "img_tensor[0,0,:,0] = 0\n",
    "print(f\"Image shape: {img_tensor.shape}\")\n",
    "print(f\"Image: \\n{img_tensor} \\n\")\n",
    "plot_image(img_tensor.reshape(img_tensor.shape[2], img_tensor.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-orleans",
   "metadata": {},
   "source": [
    "### 3.2 Create Convolutional Layer\n",
    "\n",
    "We'll use a function which expects two parameters, namely stride and padding. You can call the function with different values for both parameters to test various values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with given stride and padding\n",
    "    \n",
    "    :param stride: Stride\n",
    "    :param padding: Padding\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride = stride, padding = padding)\n",
    "\n",
    "    # Again, set the parameters of the kernel to the one from the lecture\n",
    "    kernel_parameters = torch.tensor([[-1.0, 1.0],[-1.0, 1.0]])  # Define parameter\n",
    "    conv.state_dict()['weight'][0][0]=kernel_parameters  # Set kernel\n",
    "    conv.state_dict()['bias'][0]=0.0  # Set bias to zero\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-child",
   "metadata": {},
   "source": [
    "#### Example: Stride = 1 Padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIDE = 1\n",
    "PADDING = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1_1 = create_conv_layer(STRIDE, PADDING)\n",
    "feature_map_1_1=conv_1_1(img_tensor)\n",
    "print(f\"Feature map shape: {feature_map_1_1.shape}\")\n",
    "print(f\"Feature map: \\n{feature_map_1_1} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-irrigation",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "**Task 1**: Stride=2, Padding=1. What's the size of the feature map? Before calculating, do the math by yourself.<br>\n",
    "**Task 2**: Stride=3, Padding=2. What's the size of the feature map? Before calculating, do the math by yourself.\n",
    "\n",
    "Remeber, the output shape is defined as:\n",
    "\n",
    "$$(\\frac{x_h-w_h+2p_h+s_h}{s_h},\\frac{x_w-w_w+2p_w+s_w}{s_w})$$\n",
    "\n",
    "with $(x_h,x_w)$ being the shape of the input, $(w_h,w_w)$ being the shape of the kernel, $s_h$ and $s_w$ being the height and width of the stride, and $p_h$ and $p_w$ being the height and width of padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-dollar",
   "metadata": {},
   "source": [
    "## 4. Building Block: Nonlinearity\n",
    "\n",
    "*Recap:*\n",
    "* To model nonlinear relationships we need to apply nonlinear functions, called activation functions\n",
    "* Activation functions are applied elementwise\n",
    "* Typically, CNNs deploy piecewise activation functions i.e., ReLU or other generalizations of the ReLU activation function such as LeakyReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-bradley",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-maldives",
   "metadata": {},
   "source": [
    "### 4.1 Create Feature Map\n",
    "\n",
    "We'll use the example from the lecture (see S. #42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = torch.zeros(1,1,4,4)\n",
    "feature_map[0,0,:,0:2] = 2\n",
    "feature_map[0,0,1:3,0:2] = 3\n",
    "feature_map[0,0,:,3] = -2\n",
    "feature_map[0,0,1:3,3] = -3\n",
    "print(f\"Feature map shape: {feature_map.shape}\")\n",
    "print(f\"Feature map: \\n{feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-bangladesh",
   "metadata": {},
   "source": [
    "### 4.2 Apply Activation Function\n",
    "\n",
    "PyTorch provides various activation functions including ReLU and LeakyReLU. You can find all activation functions [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "Further, note that there are actually **two** ways of applying an activation function in PyTorch. Both result in the same output.\n",
    "1. Create an object (a nn.Module class) and call it\n",
    "2. Functional (stateless) approach (torch.nn.functional). More information [here](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "\n",
    "Components:\n",
    "\n",
    "* [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "* [torch.nn.LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
    "* [torch.nn.functional.relu](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "* [torch.nn.functional.leaky_relu](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_feature_map = nn.ReLU()(feature_map)  # Create a nn.ReLU object and call it\n",
    "#nonlinear_feature_map = F.relu(feature_map)  # Functional (stateless) approach\n",
    "print(f\"Nonlinear feature map shape: {nonlinear_feature_map.shape}\")\n",
    "print(f\"Nonlinear feature map: \\n{nonlinear_feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_feature_map = nn.LeakyReLU()(feature_map)  # Create a nn.LekayReLU object and call it\n",
    "#nonlinear_feature_map = F.leaky_relu(feature_map)  # Functional (stateless) approach\n",
    "print(f\"Nonlinear feature map shape: {nonlinear_feature_map.shape}\")\n",
    "print(f\"Nonlinear feature map: \\n{nonlinear_feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-plant",
   "metadata": {},
   "source": [
    "## 5. Building Block: Pooling Operation\n",
    "\n",
    "*Recap:*\n",
    "* The pooling operation replaces the input at a certain location with a summary statistic of nearby values.\n",
    "\n",
    "Similar to *4. Building Block: Nonlinearity*, we can apply pooling operations in **two** ways. \n",
    "\n",
    "1. Creating and using a nn.Module class, or\n",
    "2. Applying a stateless function. \n",
    "\n",
    "We'll use the nn.Module approach in the following example.\n",
    "\n",
    "Different pooling functions (from the lecture):\n",
    "* Maxmimum pooling\n",
    "* Average pooling\n",
    "* Global maximum pooling\n",
    "* Global average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-aggregate",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-platform",
   "metadata": {},
   "source": [
    "### 5.1 Create Nonlinear Feature Map\n",
    "\n",
    "We'll use the example from the lecture (see S. #45,# 47, #48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_feature_map = torch.zeros(1,1,4,4)\n",
    "nonlinear_feature_map[0,0,:,0:2] = 2\n",
    "nonlinear_feature_map[0,0,1:3,0:2] = 3\n",
    "nonlinear_feature_map\n",
    "print(f\"Nonlinear feature map shape: {nonlinear_feature_map.shape}\")\n",
    "print(f\"Nonlinear feature map: \\n{nonlinear_feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-spice",
   "metadata": {},
   "source": [
    "### 5.2 Pooling\n",
    "\n",
    "In PyTorch, we can implement pooling layers by creating a nn.Module class. For Maxmimum and Average Pooling we only need to provide the following parameters: kernel_size (in the lecture we called this parameter **windows size**) and stride. For Global Maximum and Global Average Pooling we only need to provide the size of the output (in the lecture we procuded a 1x1 output, but other outputs are possible as well)\n",
    "\n",
    "Components:\n",
    "\n",
    "* [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d): Maximum Pooling\n",
    "* [torch.nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d): Average Pooling\n",
    "* [torch.nn.AdaptiveMaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d): Global maxmimum pooling\n",
    "* [torch.nn.AdaptiveAvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d): Global average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-survey",
   "metadata": {},
   "source": [
    "#### 5.2.1 Maximum Pooling\n",
    "\n",
    "* Return the maximum value within a rectangular neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "pooled_feature_map_1 = max_pooling(nonlinear_feature_map)\n",
    "pooled_feature_map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-monroe",
   "metadata": {},
   "source": [
    "#### 5.2.2 Average Pooling\n",
    "\n",
    "* Return the average value within a rectangular neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pooling = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "pooled_feature_map_2 = avg_pooling(nonlinear_feature_map)\n",
    "pooled_feature_map_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-religious",
   "metadata": {},
   "source": [
    "#### 5.2.3 Global Maximum Pooling\n",
    "\n",
    "* Return the maximum value for the entire feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_pooling = nn.AdaptiveMaxPool2d(output_size=1)\n",
    "pooled_feature_map_3 = global_max_pooling(nonlinear_feature_map)\n",
    "pooled_feature_map_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-canon",
   "metadata": {},
   "source": [
    "#### 5.2.4 Global Average Pooling\n",
    "\n",
    "* Return the average value for the entire feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "pooled_feature_map_4 = global_avg_pooling(nonlinear_feature_map)\n",
    "pooled_feature_map_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-equipment",
   "metadata": {},
   "source": [
    "## 6. Building Block: Channels\n",
    "\n",
    "*Recap*\n",
    "* CNNs can also cope with multiple input channels\n",
    "* CNNs can also produce outputs (feature maps) with multiple channels\n",
    "\n",
    "Three scenarios\n",
    "1. Multiple input channels and single output channel\n",
    "2. One input channel and multiple output channels\n",
    "3. Multiple input channels and multiple output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-division",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-wildlife",
   "metadata": {},
   "source": [
    "### 6.1 Mulitple (3) input channels and single (1) output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_OUTPUT_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-blade",
   "metadata": {},
   "source": [
    "####  6.1.1 Create an Input Image with Three Channels\n",
    "\n",
    "We'll use the example from the lecture (see S. #52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.zeros(1,NUM_INPUT_CHANNELS,4,4)\n",
    "# First channel\n",
    "image[0,0,:,:] = 1\n",
    "image[0,0,:,0] = 0\n",
    "# Second channel\n",
    "image[0,1,:,:] = 1\n",
    "image[0,1,:,1] = 0\n",
    "image[0,1,1,:] = 0\n",
    "# Third channel\n",
    "image[0,2,:,:] = 1\n",
    "image[0,2,:,3] = 0\n",
    "image[0,2,1,2] = 0\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Image: \\n{image} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-huntington",
   "metadata": {},
   "source": [
    "#### 6.1.2 Create a Convolutional Layer\n",
    "\n",
    "As we expect an **input** image with **three channels** and an **output** (feature map) with **one channel**, we will apply a kernel which has **three input channels** and **one output channel**. \n",
    "\n",
    "Intuitively, you can think of such a convolutional operation as learning a single feature across all the channels in the input. The problem is that the feature might be different from input channel to input channel, and thus, we apply a different feature for each input channel and summarize the overlap in the end. For example, while each channel in the kernel might be a vertical edge feature, they might differ for each channel of the input. The vertical edge feature for the red channel might be different than the vertical edge feature for the blue channel etc. Finally, to detect whether the whole image contains a vertical edge, we need to summarize the overlap between each feature and each channel of the input. Thus, we don't need a single channel kernel, but a multiple channel kernel (for each input channel a different channel in the kernel)\n",
    "\n",
    "We only need to change the *in_channels* and *out_channels* parameters. Here, we'll set former to three and latter to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=NUM_OUTPUT_CHANNELS, kernel_size=2, stride = 2, padding = 0)\n",
    "\n",
    "\"\"\"\n",
    "In practice you would not do that\n",
    "\n",
    "PyTorch randomly initializes the kernel. In order to illustrate the example from the lecture, we have to set the kernels parameters by ourself.\n",
    "\"\"\"\n",
    "kernel = torch.tensor([[[-1.0, 1.0],[-1.0, 1.0]], [[1.0, 1.0],[1.0, -1.0]], [[1.0, 1.0],[-1.0, 1.0]]])\n",
    "conv.state_dict()['weight'][0] = kernel\n",
    "conv.state_dict()['bias'][0] = 0.0\n",
    "\"\"\"\n",
    "Until here!\n",
    "\n",
    "In practice you would not do that\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Kernel shape: {conv.state_dict()['weight'].shape}\")\n",
    "print(f\"Kernel: \\n{conv.state_dict()['weight']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-louisiana",
   "metadata": {},
   "source": [
    "#### 6.1.3 Calculate Feature Map\n",
    "\n",
    "We can now use the convolutional layer from above to calculate the feature map given an input with three channels.\n",
    "\n",
    "What are you expecting? What is the output (feature map) shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map=conv(image)\n",
    "print(f\"Activation Map shape: {feature_map.shape}\")\n",
    "print(f\"Activation Map: \\n{feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-commonwealth",
   "metadata": {},
   "source": [
    "### 6.2 Single (1) input channels and multiple (3) output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-campaign",
   "metadata": {},
   "source": [
    "####  6.2.1 Create an Input Image with One Channels\n",
    "\n",
    "We'll use the example from the lecture (see S. #53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.zeros(1,NUM_INPUT_CHANNELS,4,4)\n",
    "# First channel\n",
    "image[0,0,:,:] = 1\n",
    "image[0,0,:,0] = 0\n",
    "image[0,0,1,2] = 0\n",
    "image[0,0,3,3] = 0\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Image: \\n{image} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-bidding",
   "metadata": {},
   "source": [
    "#### 6.2.2 Create a Convolutional Layer\n",
    "\n",
    "As we expect an **input** image with **one channel** and an **output** (feature map) with **three channels**, we will apply a kernel which has **one input channel** and **three output channels**. \n",
    "\n",
    "Intuitively, you can think of such a convolutional operation as learning *multiple, different* features in a channel simultaneously e.g., vertical edge feature, horizontal edge feature, diagonal feature etc. We measure the overlap between each feature (channel of the kernel) and the channel of the input and thus, we produce multiple channels in the feature map (for each combination of feature (channel in the kernel) and input channel we produce one channel in the feature map)\n",
    "\n",
    "We only need to change the *in_channels* and *out_channels* parameters. Here, we'll set former to one and latter to three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=NUM_OUTPUT_CHANNELS, kernel_size=2, stride = 2, padding = 0)\n",
    "\n",
    "\"\"\"\n",
    "In practice you would not do that\n",
    "\n",
    "PyTorch randomly initializes the kernel. In order to illustrate the example from the lecture, we have to set the kernels parameters by ourself.\n",
    "\"\"\"\n",
    "kernel = []\n",
    "kernel.append(torch.tensor([[1.0, 1.0],[-1.0, 1.0]]))\n",
    "kernel.append(torch.tensor([[1.0, 1.0],[1.0, -1.0]]))\n",
    "kernel.append(torch.tensor([[-1.0, 1.0],[-1.0, 1.0]]))\n",
    "for channel in range(NUM_OUTPUT_CHANNELS):\n",
    "    conv.state_dict()['weight'][channel][0]=kernel[channel]\n",
    "    conv.state_dict()['bias'][channel]=0.0\n",
    "\"\"\"\n",
    "Until here!\n",
    "\n",
    "In practice you would not do that\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Kernel shape: {conv.state_dict()['weight'].shape}\")\n",
    "print(f\"Kernel: \\n{conv.state_dict()['weight']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-motorcycle",
   "metadata": {},
   "source": [
    "#### 6.2.3 Calculate Feature Map\n",
    "\n",
    "We can now use the convolutional layer from above to calculate the feature map with three channels given an input with one channel.\n",
    "\n",
    "What are you expecting? What is the output (feature map) shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map=conv(image)\n",
    "print(f\"Activation Map shape: {feature_map.shape}\")\n",
    "print(f\"Activation Map: \\n{feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-greensboro",
   "metadata": {},
   "source": [
    "### 6.3 Multiple (2) input channels and multiple (2) output channel\n",
    "\n",
    "It turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go higher up in the neural network, typically downsampling to trade of spatial resolution for greater channel depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 2\n",
    "NUM_OUTPUT_CHANNELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-illustration",
   "metadata": {},
   "source": [
    "####  6.3.1 Create an Input Image with Two Channels\n",
    "\n",
    "We'll use the example from the lecture (see S. #54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.zeros(1,NUM_INPUT_CHANNELS,4,4)\n",
    "# First channel\n",
    "image[0,0,:,:] = 1\n",
    "image[0,0,:,0] = 0\n",
    "# Second channel\n",
    "image[0,1,:,:] = 1\n",
    "image[0,1,:,1] = 0\n",
    "image[0,1,1,:] = 0\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Image: \\n{image} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-savannah",
   "metadata": {},
   "source": [
    "#### 6.3.2 Create a Convolutional Layer\n",
    "\n",
    "As we expect an **input** image with **two channels** and an **output** (feature map) with **two channels**, we will apply a kernel which has **two input channels** and **two output channels**. \n",
    "\n",
    "This is a combination of 6.1 and 6.2. Intuitively, you can think of such a convolutional operation as learning *multiple, different* features simultaneously (6.2) *across* multiple input channels (6.1). For example, for each input channel (2) we detect two different features (e.g., vertical and horizontal edge)\n",
    "\n",
    "We only need to change the *in_channels* and *out_channels* parameters. Here, we'll set former to two and latter to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=NUM_OUTPUT_CHANNELS, kernel_size=2, stride = 2, padding = 0)\n",
    "\n",
    "\"\"\"\n",
    "In practice you would not do that\n",
    "\n",
    "PyTorch randomly initializes the kernel. In order to illustrate the example from the lecture, we have to set the kernels parameters by ourself.\n",
    "\"\"\"\n",
    "kernel = []\n",
    "output_kernels_0 = []\n",
    "# First dimension\n",
    "output_kernels_0.append(torch.tensor([[-1.0, 1.0],[-1.0, 1.0]]))\n",
    "output_kernels_0.append(torch.tensor([[1.0, 1.0],[1.0, -1.0]]))\n",
    "kernel.append(output_kernels_0)\n",
    "# Second dimension\n",
    "output_kernels_1 = []\n",
    "output_kernels_1.append(torch.tensor([[-1.0, 1.0],[1.0, -1.0]]))\n",
    "output_kernels_1.append(torch.tensor([[1.0, 1.0],[1.0, 1.0]]))\n",
    "kernel.append(output_kernels_1)\n",
    "\n",
    "for out_channel in range(NUM_OUTPUT_CHANNELS):\n",
    "    for in_channel in range(NUM_INPUT_CHANNELS):\n",
    "        conv.state_dict()['weight'][out_channel][in_channel]=kernel[out_channel][in_channel]\n",
    "        conv.state_dict()['bias'][out_channel]=0.0\n",
    "\"\"\"\n",
    "Until here!\n",
    "\n",
    "In practice you would not do that\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Kernel shape: {conv.state_dict()['weight'].shape}\")\n",
    "print(f\"Kernel: \\n{conv.state_dict()['weight']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-webster",
   "metadata": {},
   "source": [
    "#### 6.3.3 Calculate Feature Map\n",
    "\n",
    "We can now use the convolutional layer from above to calculate the feature map with two channels given an input with two channel.\n",
    "\n",
    "What are you expecting? What is the output (feature map) shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map=conv(image)\n",
    "print(f\"Activation Map shape: {feature_map.shape}\")\n",
    "print(f\"Activation Map: \\n{feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-upper",
   "metadata": {},
   "source": [
    "## 7. Building Block: Classification\n",
    "\n",
    "*Recap:*\n",
    "* Flatten the output into a two-dimensional representation and apply (multiple) fully-connected layer(s) to produce an n-dimensional output which corresponds to the number of possible output classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-prior",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-breast",
   "metadata": {},
   "source": [
    "####  7.1 Create a Pooled Feature Map\n",
    "\n",
    "We'll use the example from the lecture (see S. #60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_feature_map = torch.tensor([[[2.5, 3.5], [2.5, 1.5]], [[0.7, 1.7], [0.7, 3.7]], [[2.3, 1.3], [2.3, -0.7]]])\n",
    "pooled_feature_map = pooled_feature_map.reshape(1, pooled_feature_map.shape[0], pooled_feature_map.shape[1], pooled_feature_map.shape[2])\n",
    "print(\"Image shape: {pooled_feature_map.shape}\")\n",
    "print(f\"Image: \\n{pooled_feature_map} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-employment",
   "metadata": {},
   "source": [
    "####  7.2 Flatten\n",
    "\n",
    "Transform the input such that the shape is two-dimensional\n",
    "\n",
    "Components:\n",
    "\n",
    "* [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_pooled_feature_map = torch.flatten(pooled_feature_map, 1)\n",
    "print(f\"Flatten pooled feature map shape: {flatten_pooled_feature_map.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-newport",
   "metadata": {},
   "source": [
    "#### 7.3 Fully-Connected Layers\n",
    "\n",
    "Let's assume you want to have one fully-connected layer which produces two output neurons. \n",
    "\n",
    "Note that in PyTorch fully-connected layers are called Linear Layers.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 2\n",
    "\n",
    "fc = nn.Linear(in_features=flatten_pooled_feature_map.shape[1], out_features=num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-eating",
   "metadata": {},
   "source": [
    "#### 7.4 Calculate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fc(flatten_pooled_feature_map)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: \\n{output} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-management",
   "metadata": {},
   "source": [
    "## 8. Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-dealing",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-coordinate",
   "metadata": {},
   "source": [
    "#### Set random seeds\n",
    "\n",
    "To make PyTorch reproducable, we need to set seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.cuda.manual_seed_all(123) \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-attention",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_from_url(url):\n",
    "    \"\"\"\n",
    "    Reading images from url\n",
    "    \n",
    "    :param url: URL\n",
    "    :return raw image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-thursday",
   "metadata": {},
   "source": [
    "### 8.1 Load image\n",
    "\n",
    "Components\n",
    "\n",
    "* [transforms.ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor): Convert an array to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGR is the default color space when reading images with cv2\n",
    "img_raw = read_image_from_url(\"https://raw.githubusercontent.com/jmelsbach/ai-im/main/data/cnns/happy_child.jpg\")\n",
    "img_rgb = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)  # Cast into RGB image\n",
    "\n",
    "tran = transforms.ToTensor() \n",
    "img_tensor = tran(img_rgb)\n",
    "img_tensor = img_tensor.reshape(1,3,img_rgb.shape[0],img_rgb.shape[1])  # Keep in mind, the shape is (number of images, number of channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-facial",
   "metadata": {},
   "source": [
    "### 8.2 Create CNN\n",
    "\n",
    "We'll implement the simply CNN from the lecture (see S. #62) using the building blocks described above. Note that the values of the parameters (within the kernels) are not the same than the one from the lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCnn, self).__init__()\n",
    "        # Create first convolutional layer\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride = 2, padding = 0)\n",
    "        # Create second convolutional layer\n",
    "        self.conv_2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride = 2, padding = 0)\n",
    "        # Create max pooling layer\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # Fully-connected Layer\n",
    "        self.fc = nn.Linear(in_features=6936, out_features=2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        feature_map_1 = self.conv_1(X)                                          #3x277x281\n",
    "        nonlinear_feature_map_1 = F.relu(feature_map_1)                         #3x277x281\n",
    "        pooled_nonlinear_feature_map_1 = self.pooling(nonlinear_feature_map_1)  #3x138x140\n",
    "        feature_map_2 = self.conv_2(pooled_nonlinear_feature_map_1)             #6x68x69\n",
    "        nonlinear_feature_map_2 = F.relu(feature_map_2)                         #6x68x69\n",
    "        pooled_nonlinear_feature_map_2 = self.pooling(nonlinear_feature_map_2)  #6x34x34\n",
    "        feature_map_flattened = pooled_nonlinear_feature_map_2.flatten(1)       #1x6936\n",
    "        output = self.fc(feature_map_flattened)                                 #1x2\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = SimpleCnn()  # Create object from class SimpleCnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-moderator",
   "metadata": {},
   "source": [
    "### 8.3 Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn(img_tensor)\n",
    "print(output.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-charter",
   "metadata": {},
   "source": [
    "## 9. Questions\n",
    "\n",
    "1. What is a \"feature\"?\n",
    "2. Write out the convolutional kernel matrix for a top edge detector.\n",
    "3. What is padding?\n",
    "4. What is stride?\n",
    "5. What is Flatten? Where is it needed?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: aiim_env]",
   "language": "python",
   "name": "aiim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
