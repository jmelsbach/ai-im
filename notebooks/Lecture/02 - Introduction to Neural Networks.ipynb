{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johannesmelsbach/ai-im/blob/main/notebooks/Lecture/02%20-%20Introduction%20to%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02 Notebook - Introduction to Neural Networks\n",
    "\n",
    "The notebook is accomponying Lecture 02 - Introduction to Neural Networks.\n",
    "\n",
    "This and the following notebooks accompany the lecture to show you how to implement neural networks and the like using PyTorch & Co with relatively little effort. You will see that most of the concepts, techniques and functions are already available and it is often very easy to make use of them. All notebooks will be connected to [Colab](https://colab.research.google.com/notebooks/intro.ipynb) such that you can directly execute the code and play with it by yourself. We encourage you to not just execute the code but to also think about it in detail. \n",
    "\n",
    "This notebook should give you a first impression of how to implement very basic Neural Networks. We'll implement/use some of the terminologies and techniques used in the lecture such as a fordward pass with and without activation functions. \n",
    "\n",
    "Have fun & keep coding!\n",
    "\n",
    "Authors: Johannes Melsbach & Jannik Rößler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Pass without Activation Function\n",
    "\n",
    "Let's get started with building a simple Neural Network with one hidden layer. We will use the example and the weights from the lecture to illustrate how a forward pass is working (without activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a Neural Network\n",
    "\n",
    "Define a custom Module using PyTorchs' Module subclass\n",
    "\n",
    "While we define input, hidden and output layers as well as other components of the Network in the *init* function, we implement the forward loop inside the *forward* function. Note that we use a fully connected layer (a fully connected layer is a layer where each neuron is connected to each neuron of the previous layer) as hidden layer.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        z2 = self.output_layer(z1)\n",
    "        return z2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Custom Weights\n",
    "\n",
    "We can have a look at the weights of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are initialized randomly. We will replace the weights of the network to match the example in the lecture. Of course this wouldn't be done in practice and is just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our Training example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training example\n",
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed the `training_example` example into the neural network by using the network like a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "### 2.1 Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(a))\n",
    "print(sigmoid(b))\n",
    "print(sigmoid(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relu(a))\n",
    "print(relu(b))\n",
    "print(relu(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tanh(a))\n",
    "print(tanh(b))\n",
    "print(tanh(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.tensor([0.25, 3.55, 0.85, 0.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the tensor as well as the dimension which should be transformed. Here, we are using a tensor with only one dimension, and thus, we want to apply the Softmax on dimension 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(o, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Linear Functions\n",
    "\n",
    "@Johannes: Please add purpose of this code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return 3 * x + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_f2(x):\n",
    "    return f2(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    return 6 * x + 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f1_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Pass with Activation Function\n",
    "\n",
    "As we said in the lecture, we need to introduce non-linearity to tacke non-linear problems. Thus, let's integrate an activation function (here: Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a Neural Network\n",
    "\n",
    "The Neural Network architecture is very similar to the one above, except that we now use a Sigmoid activation function in our forward loop to transform the input.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer\n",
    "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid): Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        z2 = self.output_layer(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        \n",
    "        return a2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add Custom Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. See [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change weights again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "So far, we learned that training a neural network consists of 4 steps:\n",
    "\n",
    "1. Forward Pass\n",
    "2. Calculate Loss\n",
    "3. Calculate Gradients\n",
    "4. Optimize the Weights\n",
    "\n",
    "Training a neural networks usually requires that we go over and over these steps until the network converges. In the following we will give you a very simple example of how to implement such a training loop, using all the concept we have learned in the lecture. Note that this is not a best practice example, rather it is a very first start to understand what a training loop in PyTorch looks like.\n",
    "\n",
    "### 4.1 The Data set\n",
    "\n",
    "First, we will create a data set which contains information about the students who took the AIIM course last year (of course we made these numbers up!). You can think of the target variable (y) as the percentage for correct tasks in the exam , while the features x1 and x2 can be seen as how many hours the students worked on the lectures and excersice notebooks, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Features: Generate random numbers for x1 and x2 (between 0 and 100)\n",
    "x = np.random.rand(100, 2) * 100\n",
    "\n",
    "# Target\n",
    "y = 0.01 * np.random.randn(100) + np.log(x[:,0]) + np.log(x[:,1])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y = scaler.fit_transform(y.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Training, Validation and Test Split\n",
    "\n",
    "We will create a training, validation and test set with the following ratio: 60:20:20. The training set will be used for updating the neural networks parameters. The validation set will be used to prevent overfitting while our final evaulation will be on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create Training (60%) and Validatation/Test Split (40%)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.40, random_state=seed)\n",
    "\n",
    "# Second, create Validation (50%) and Test Split (50%)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.50, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor\n",
    "\n",
    "# Training\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "\n",
    "# Validation\n",
    "x_valid = torch.from_numpy(x_valid).type(torch.FloatTensor)\n",
    "y_valid = torch.from_numpy(y_valid).type(torch.FloatTensor)\n",
    "\n",
    "# Test\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorDataset\n",
    "\n",
    "PyTorch’s TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to **access both the independent and dependent variables** in the same line as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "test_ds = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "\n",
    "Pytorch’s DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training DataLoader\n",
    "BATCH_SIZE = 2\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE*2, shuffle=True)  # We can increase the batch size of validation because of less computational complexity\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=True)  # We can increase the batch size of test because of less computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create Neural Network\n",
    "\n",
    "Before you look at the code, think about what the network should look like? How many input neurons do you have? How many output neurons do you have? What is the activation function in your output layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer\n",
    "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid): Sigmoid activation function\n",
    "* [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html): Flatten function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(in_features=2, out_features=5)\n",
    "        self.output_layer = nn.Linear(in_features=5, out_features=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        a1 = torch.relu(z1)\n",
    "        z2 = self.output_layer(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        \n",
    "        # TODO: Check the shape of the tensor yourself using a2.shape before and after calling the flatten method. What changes?\n",
    "        y_hat = torch.flatten(a2)  # Flatten a tensor such that it has a single dimension\n",
    "        return y_hat\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs (the number of training iterations over the entire data set)\n",
    "n_epochs = 10\n",
    "# Define the optimizer. Here we will use Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "# We will use MSE as our loss function\n",
    "loss_func = nn.MSELoss()\n",
    "# Flag for printing\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the losses of each epoch for plotting purposes\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "\n",
    "# Iterate over the dataset n_epochs times\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()  # net.eval() will notify all your layers that you are in training mode\n",
    "    # Sum of all losses (of all batches in one epoch)\n",
    "    batches_train_loss = 0\n",
    "    # For each batch, pass the training examples, calculate loss and gradients and optimize the parameters\n",
    "    for xb, yb in train_dl:\n",
    "        # TODO: Check yourself what xb and yb actually look like. What shapes are you expecting?\n",
    "        optimizer.zero_grad()  # zero_grad clears old gradients from the last step\n",
    "        y_hat = net(xb)  # Forward pass\n",
    "        loss = loss_func(y_hat, yb)  # Calculate Loss\n",
    "        loss.backward()  # Calculate the gradients (using backpropagation)\n",
    "        optimizer.step()  # # Optimize the parameters: opt.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "        batches_train_loss += loss.item()\n",
    "    \n",
    "    batches_valid_loss = 0\n",
    "    net.eval()  # net.eval() will notify all your layers that you are in evaluation mode\n",
    "    # torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed \n",
    "    # up computations but you won’t be able to backprop (which you don’t want in an evaluation script).\n",
    "    with torch.no_grad():\n",
    "        # Perform a prediction on the validation set  \n",
    "        for xb_valid, yb_valid in valid_dl:\n",
    "\n",
    "            y_hat = net(xb_valid)  # Forward pass\n",
    "            loss = loss_func(y_hat, yb_valid)  # Calculate Loss\n",
    "            batches_valid_loss += loss.item()\n",
    "        \n",
    "    if VERBOSE:\n",
    "        print(f\"Train Loss (MSE) in epoch {epoch}: {batches_train_loss:.2f}\")\n",
    "        print(f\"Validation Loss (MSE) in epoch {epoch}: {batches_valid_loss:.2f}\\n\")\n",
    "    loss_train_list.append(batches_train_loss)\n",
    "    loss_valid_list.append(batches_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(loss_train_list, color='red', label='Training Loss')\n",
    "ax1.plot(loss_valid_list, color='blue', label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Test Performance\n",
    "\n",
    "**Important:** Only execute the following steps, if you are satisfied with your model (performance on training and validation set is well and no sign of overfitting or underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    # Perform a prediction on the test set  \n",
    "    for xb_test, yb_test in test_dl:\n",
    "\n",
    "        y_hat = net(xb_test)  # Forward pass\n",
    "        loss = loss_func(y_hat, yb_test)  # Calculate Loss\n",
    "        batches_test_loss += loss.item()\n",
    "\n",
    "print(f\"Test MSE is: {batches_test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Although the example above is only a mini working example, it should give you a first impression of how to write a training loop in PyTorch. We will learn to improve the code in the next lectures. To start with, ask yourself: How can you improve the MSE Loss? What are possible opportunities (adjusting screws) to increase performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Questions\n",
    "\n",
    "1. Why do we need activation functions?\n",
    "2. What activation function should be used in the output layer?\n",
    "3. What does nn.Linear do in PyTorch? What parameters is the function expecting?\n",
    "4. Which activation function should be used in multi-label classification\n",
    "5. Why is the Sigmoid activation function depreciated in feed-forward neural networks?\n",
    "\n",
    "@Johannes: Please add further questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: aiim_env]",
   "language": "python",
   "name": "aiim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
