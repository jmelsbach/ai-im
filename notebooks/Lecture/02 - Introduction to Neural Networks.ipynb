{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johannesmelsbach/ai-im/blob/main/notebooks/Lecture/02%20-%20Introduction%20to%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02 Notebook - Introduction to Neural Networks\n",
    "\n",
    "The notebook is accomponying Lecture 02 - Introduction to Neural Networks.\n",
    "\n",
    "This and the following notebooks accompany the lecture to show you how to implement neural networks and the like using PyTorch & Co with relatively little effort. You will see that most of the concepts, techniques and functions are already available and it is often very easy to make use of them. All notebooks will be connected to [Colab](https://colab.research.google.com/notebooks/intro.ipynb) such that you can directly execute the code and play with it by yourself. We encourage you to not just execute the code but to also think about it in detail. \n",
    "\n",
    "This notebook should give you a first impression of how to implement very basic Neural Networks. We'll implement/use some of the terminologies and techniques used in the lecture such as a fordward pass with and without activation functions. \n",
    "\n",
    "Have fun & keep coding!\n",
    "\n",
    "Authors: Johannes Melsbach & Jannik Rößler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Pass without Activation Function\n",
    "\n",
    "Let's get started with building a simple Neural Network with one hidden layer. We will use the example and the weights from the lecture to illustrate how a forward pass is working (without activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a NeuralNetwork\n",
    "\n",
    "Define a custom Module using PyTorchs' Module subclass\n",
    "\n",
    "While we define input, hidden and output layers as well as other components of the Network in the *init* function, we implement the forward loop inside the *forward* function. Note that we use a fully connected layer (a fully connected layer is a layer where each neuron is connected to each neuron of the previous layer) as hidden layer.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        z2 = self.output_layer(z1)\n",
    "        return z2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Custom Weights\n",
    "\n",
    "We can have a look at the weights of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are initialized randomly. We will replace the weights of the network to match the example in the lecture. Of course this wouldn't be done in practice and is just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our Training example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training example\n",
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed the `training_example` example into the neural network by using the network like a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "### 2.1 Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(a))\n",
    "print(sigmoid(b))\n",
    "print(sigmoid(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relu(a))\n",
    "print(relu(b))\n",
    "print(relu(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tanh(a))\n",
    "print(tanh(b))\n",
    "print(tanh(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.tensor([0.25, 3.55, 0.85, 0.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the tensor as well as the dimension which should be transformed. Here, we are using a tensor with only one dimension, and thus, we want to apply the Softmax on dimension 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(o, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Linear Functions\n",
    "\n",
    "@Johannes: Please add purpose of this code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return 3 * x + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_f2(x):\n",
    "    return f2(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    return 6 * x + 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f1_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Pass with Activation Function\n",
    "\n",
    "As we said in the lecture, we need to introduce non-linearity to tacke non-linear problems. Thus, let's integrate an activation function (here: Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a Neural Network\n",
    "\n",
    "The Neural Network architecture is very similar to the one above, except that we now use a Sigmoid activation function in our forward loop to transform the input.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer\n",
    "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid): Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        z2 = self.output_layer(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        \n",
    "        return a2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add Custom Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. See [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change weights again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Questions\n",
    "\n",
    "1. Why do we need activation functions?\n",
    "2. What activation function should be used in the output layer?\n",
    "3. What does nn.Linear do in PyTorch? What parameters is the function expecting?\n",
    "\n",
    "@Johannes: Please add further questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: aiim_env]",
   "language": "python",
   "name": "aiim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
