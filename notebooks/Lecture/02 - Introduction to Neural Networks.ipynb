{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johannesmelsbach/ai-im/blob/main/notebooks/Lecture/02%20-%20Introduction%20to%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02 Notebook - Introduction to Neural Networks\n",
    "\n",
    "The notebook is accomponying Lecture 02 - Introduction to Neural Networks.\n",
    "\n",
    "This and the following notebooks accompany the lecture to show you how to implement neural networks and the like using PyTorch & Co with relatively little effort. You will see that most of the concepts, techniques and functions are already available and it is often very easy to make use of them. All notebooks will be connected to [Colab](https://colab.research.google.com/notebooks/intro.ipynb) such that you can directly execute the code and play with it by yourself. We encourage you to not just execute the code but to also think about it in detail. \n",
    "\n",
    "This notebook should give you a first impression of how to implement very basic Neural Networks. We'll implement/use some of the terminologies and techniques used in the lecture such as a fordward pass with and without activation functions. \n",
    "\n",
    "Have fun & keep coding!\n",
    "\n",
    "Authors: Johannes Melsbach & Jannik Rößler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Pass without Activation Function\n",
    "\n",
    "Let's get started with building a simple Neural Network with one hidden layer. We will use the example and the weights from the lecture to illustrate how a forward pass is working (without activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a Neural Network\n",
    "\n",
    "Define a custom Module using PyTorchs' Module subclass\n",
    "\n",
    "While we define input, hidden and output layers as well as other components of the Network in the *init* function, we implement the forward loop inside the *forward* function. Note that we use a fully connected layer (a fully connected layer is a layer where each neuron is connected to each neuron of the previous layer) as hidden layer.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        z2 = self.output_layer(z1)\n",
    "        return z2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Custom Weights\n",
    "\n",
    "We can have a look at the weights of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are initialized randomly. We will replace the weights of the network to match the example in the lecture. Of course this wouldn't be done in practice and is just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden_layer.weight, net.output_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our Training example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training example\n",
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed the `training_example` example into the neural network by using the network like a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "### 2.1 Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(a))\n",
    "print(sigmoid(b))\n",
    "print(sigmoid(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relu(a))\n",
    "print(relu(b))\n",
    "print(relu(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(-4.)\n",
    "b = torch.tensor(3.)\n",
    "c = torch.tensor(8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tanh(a))\n",
    "print(tanh(b))\n",
    "print(tanh(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.tensor([0.25, 3.55, 0.85, 0.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the tensor as well as the dimension which should be transformed. Here, we are using a tensor with only one dimension, and thus, we want to apply the Softmax on dimension 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(o, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Linear Functions\n",
    "\n",
    "@Johannes: Please add purpose of this code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(func):\n",
    "    x = torch.tensor(np.linspace(-8.,8., 100))\n",
    "    y = torch.tensor([func(value) for value in x])\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return 3 * x + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_f2(x):\n",
    "    return f2(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    return 6 * x + 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f1_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_function(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Pass with Activation Function\n",
    "\n",
    "As we said in the lecture, we need to introduce non-linearity to tacke non-linear problems. Thus, let's integrate an activation function (here: Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a Neural Network\n",
    "\n",
    "The Neural Network architecture is very similar to the one above, except that we now use a Sigmoid activation function in our forward loop to transform the input.\n",
    "\n",
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer\n",
    "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid): Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Create Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(in_features=3, out_features=3, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "        # Create Output Layer\n",
    "        self.output_layer = nn.Linear(in_features=3, out_features=2, bias=False)  # For demonstration purposes we will ignore the bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        z2 = self.output_layer(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        \n",
    "        return a2\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add Custom Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. See [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change weights again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = nn.Parameter(torch.tensor([[0.5, 1.0, -0.5],[0.75, -1.0, 0.5],[0.25, -0.5, 0.5]]))\n",
    "output_weights = nn.Parameter(torch.tensor([[0.25, 0.25, 0.1], [0.25, 0.5, 0.5]]))\n",
    "\n",
    "net.hidden_layer.weight = hidden_weights\n",
    "net.output_layer.weight = output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_example = torch.tensor([1., 0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "So far, we learned that training a neural network consists of 4 steps:\n",
    "\n",
    "1. Forward Pass\n",
    "2. Calculate Loss\n",
    "3. Calculate Gradients\n",
    "4. Optimize the Weights\n",
    "\n",
    "Training a neural networks usually requires that we go over and over these steps until the network converges. In the following we will give you a very simple example of how to implement such a training loop, using all the concept we have learned in the lecture. Note that this is not a best practice example, rather it is a very first start to understand what a training loop in PyTorch looks like.\n",
    "\n",
    "### 4.1 The Data set\n",
    "\n",
    "First, we will create a data set which contains information about the students who took the AIIM course last year (of course we made these numbers up!). While the target variable (y) represents the grade of the students, the features x1, and x2 describe how many hours the students worked on the lecture and exercise notebooks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features (10 x 2 Tensor => 10 observations with 2 features each)\n",
    "x = torch.tensor(([2, 7], [10, 10], [4, 8], [19, 16], [10, 23], [16, 15], [2, 4], [14, 20], [20, 7], [8, 11]), dtype=torch.float)\n",
    "# Create target\n",
    "y = torch.tensor((5.0, 2.7, 3.7, 1.3, 1.7, 1.3, 5.0, 1.7, 2.3, 2.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch’s TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to **access both the independent and dependent variables** in the same line as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "\n",
    "train_ds = TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "\n",
    "Pytorch’s DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training DataLoader\n",
    "BATCH_SIZE = 2\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Neural Network\n",
    "\n",
    "Before you look at the code, think about what the network should look like? How many input neurons do you have? How many output neurons do you have? What is the activation function in your output layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear): Creates a Fully-Connected Layer\n",
    "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid): Sigmoid activation function\n",
    "* [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html): Flatten function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(in_features=2, out_features=10)\n",
    "        self.output_layer = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = self.hidden_layer(X)\n",
    "        a1 = torch.relu(z1)\n",
    "        z2 = self.output_layer(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        \n",
    "        # TASK: Check the shape of the tensor yourself using a2.shape before and after calling the flatten method. What changes?\n",
    "        y_hat = torch.flatten(a2)  # Flatten a tensor such that it has a single dimension\n",
    "        return y_hat\n",
    "    \n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs (the number of training iterations over the entire data set)\n",
    "N_EPOCHS = 3\n",
    "# Define the optimizer. Here we will use Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "# We will use MSE as our loss function\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataset n_epochs times\n",
    "for epoch in range(n_epochs):\n",
    "    # Sum of all losses (of all batches in one epoch)\n",
    "    overall_loss = 0\n",
    "    # For each batch, pass the training examples, calculate loss and gradients and optimize the parameters\n",
    "    for xb, yb in train_dl:\n",
    "        # TASK: Check yourself what xb and yb actually look like. What shape are you expecting?\n",
    "        optimizer.zero_grad()  # zero_grad clears old gradients from the last step\n",
    "        y_hat = net(xb)  # Forward pass\n",
    "        loss = loss_func(y_hat, yb)  # Calculate Loss\n",
    "        loss.backward()  # Calculate the gradients (using backpropagation)\n",
    "        optimizer.step()  # # Optimize the parameters: opt.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "    print(f\"Train Loss in epoch #{epoch}: {overall_loss:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Although the example above isn't comprehensive it should give you a first impression of how to write a training loop in PyTorch. We will learn to improve the code in the next lectures. To start with, ask yourself: How can you improve the MSE Loss? What is missing in the code above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Questions\n",
    "\n",
    "1. Why do we need activation functions?\n",
    "2. What activation function should be used in the output layer?\n",
    "3. What does nn.Linear do in PyTorch? What parameters is the function expecting?\n",
    "4. Which activation function should be used in multi-label classification\n",
    "5. Why is the Sigmoid activation function depreciated in feed-forward neural networks?\n",
    "\n",
    "@Johannes: Please add further questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: aiim_env]",
   "language": "python",
   "name": "aiim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
