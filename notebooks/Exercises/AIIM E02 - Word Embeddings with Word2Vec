{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIIM E02 - Word Embeddings with Word2Vec","provenance":[{"file_id":"1mmSnOzsp8z-0Qd39VQDDbz6IFbp3e3HS","timestamp":1620469955482}],"authorship_tag":"ABX9TyO2PvLhTh1MaSwPTITc77S0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8OSoLnKRMZQS"},"source":["![Logo Uni KÃ¶ln](https://raw.githubusercontent.com/jmelsbach/ai-im/main/img/uni-logo.png)"]},{"cell_type":"markdown","metadata":{"id":"8L0w5U9pMdcx"},"source":["# Exercise 02 Notebook - Preprocessing and Word2Vec\n"]},{"cell_type":"markdown","metadata":{"id":"0rW011jzMqlp"},"source":["In this exercise you will create vector representations of words and documents. \n","In the python eco system there are several libraries that make it very easy to implement to achieve this.\n","\n","In this notebook we will use the  `gensim` library for text preprocessing and the training of a `word2Vec` model."]},{"cell_type":"markdown","metadata":{"id":"IlKXi8Q4_47v"},"source":["## 1. Using a pretrained Word2Vec Model\n","In this part we will download a pretrained word2vec model that was trained on a huge news corpus by google. The model was trained in 2013 so the data includes news articles from before that year. The model is quite large so downloading it will take about 10 minutes."]},{"cell_type":"code","metadata":{"id":"Gg5SeSjqz5_h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsxYE0mbNA9M"},"source":["### 1.1 Exploring the model"]},{"cell_type":"markdown","metadata":{"id":"Sfjj8oagIRGi"},"source":["We saw in the lecture that it can useful to preprocess the text data before training a vectorization technique. Try to find out by finding examples how the data in the google corpus has been preprocessed. Things you should check:\n","* lowercasing\n","* stopword removal\n","* stemming\n","* n-grams"]},{"cell_type":"code","metadata":{"id":"qEkDDvu3IreI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMIWsgXFDR49"},"source":["As you are now more familar with the model complete the following tasks:\n","* v(king) - v(man) + v(woman) = ?\n","* use analogies to find out the capital 'Cameroon'\n","* use analogies to find out who was the prime minister of spain\n","* find other examples where this works out and makes sense\n","* find examples where this doesn't work well\n","* use the word2vec model to find out which of the following words does not fit in the group: `[\"breakfast\", \"cereal\", \"dinner\", \"lunch\"]`"]},{"cell_type":"code","metadata":{"id":"wVrQqceehSwK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yB3UIoiUiiw-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDQMOv69iisK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htUYaobuPGMv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBrjR842JMBg"},"source":["### 1.2 Bias in word embeddings\n","Word2Vec learns word relationships on a training corpus. If there is a bias in the training corpus, there is a great chance that those are also reflected in the resulting word embeddings. [This Paper](https://arxiv.org/pdf/1607.06520.pdf) has studied bias in word2vec word embeddings. Look at the examples given in the paper and try if you can reproduce any of them."]},{"cell_type":"code","metadata":{"id":"jBQCY1iSDte6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPvajZnBAC-Z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2ipgquWNTm0"},"source":["### 1.3 Visualizing Word Embeddings\n","Word Embeddings usually live in a very high dimensional space. Visualizing data with more than three dimensions is very difficult and becomes impossible very quickly.\n","\n","If we want to visualize a high dimensional vector we have to use so called dimensionality reduction techniques that are able to reduce the dimensions of a group of vectors and preserve relative properties. It is needless to say that you lose a lot of information if you reduce the dimensionality but is is a good way to visualize the relation between word embeddings. The following code uses the so called `principal component analysis (PCA)`  to make 2-dimensional vectors. Click [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for details about the implementation."]},{"cell_type":"code","metadata":{"id":"o0YP2TyRPVeO"},"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","np.random.seed(0)\n","def display_pca_scatterplot(model, words):\n","\n","        \n","    word_vectors = np.array([model.wv[w] for w in words])\n","\n","    twodim = PCA().fit_transform(word_vectors)[:,:2]\n","    \n","    plt.figure(figsize=(20,12))\n","    plt.rcParams.update({'font.size': 18})\n","    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n","    for word, (x,y) in zip(words, twodim):\n","        plt.text(x+0.05, y+0.05, word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HMPC7iCPGDW"},"source":["Create a list of about 20 words you would like to plot in the diagramm."]},{"cell_type":"code","metadata":{"id":"EhA7lt4EPfeq"},"source":["word_list = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1OXnJo9cPPUS"},"source":["Run the `display_pca_scatterplot` function and interpret the results."]},{"cell_type":"code","metadata":{"id":"VthHKW5lPYRO"},"source":["display_pca_scatterplot(wv, word_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OP2N9osVmBMY"},"source":["## 2. Training a Word2Vec Model\n","In this part we will train our first word2vec model by our self using the gensim library. We will train our model on the text8 dataset which we can download directly with the gensim library. The dataset is already in the right format to train a model on it."]},{"cell_type":"code","metadata":{"id":"efvi52UNJwP3"},"source":["import gensim\n","import gensim.downloader as api\n","from gensim.models import Word2Vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90Qf3VExKot_"},"source":["# text8 corpus consists of wikipedia data from the year 2006\n","# http://mattmahoney.net/dc/textdata.html\n","corpus = api.load('text8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nCoYK6fQLEuv"},"source":["Look into the word2vec [documentation](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) of the gensim library and to the following tasks:\n","\n","Train a Word2Vec Model with the following hyperparameters:\n","  * vector size of 100\n","  * window size of 5\n","  * negative_sampling of 3\n","\n","  The training of the model on the text8 dataset should take about 2 minutes."]},{"cell_type":"code","metadata":{"id":"NVIKEiyiOcDi"},"source":["# train word2vec model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivfJ5mE_SaJg"},"source":["### 2.1 Exploring the model"]},{"cell_type":"markdown","metadata":{"id":"BhKqulIpPoAz"},"source":["The resulting model is much smaller than the pretrained model we used previously. Solve the same tasks as in Section 1. Does it work as well?\n","\n"]},{"cell_type":"code","metadata":{"id":"smpuTnS1Zill"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3cy3NUiZv3F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQnUGHqcZjPv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWUQjLZUaIBN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sr_8UQc3Smgy"},"source":["### 2.2 Visualizing Word Embeddings\n","Once again visualize a list of words. You can use the list you created in Section 1 but you might have to make some adjustments."]},{"cell_type":"code","metadata":{"id":"PmWV9bN-PHKn"},"source":["# example list\n","word_list = ['coffee', 'tea', 'beer', 'wine', 'water',                                      # Beverages\n","             'spaghetti', 'hamburger', 'pizza',                                             # Food\n","             'dog', 'horse', 'cat', 'mouse',                                                # Animals\n","             'france', 'germany', 'hungary', 'china',                                       # countries\n","             'school', 'college', 'university', 'institute',                                # Education\n","             'soccer', 'basketball', 'baseball', 'football']                                # Sports"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Slhou_DmQUuM"},"source":["display_pca_scatterplot(model, word_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"az_3IBqnSHrk"},"source":["## 3. Training a word2vec model with a custom dataset"]},{"cell_type":"markdown","metadata":{"id":"h5in3NJDBF1a"},"source":["In this section we will train word embeddings on our own dataset. We will preprocess the data by our selfes and will therefore have full control of the process."]},{"cell_type":"markdown","metadata":{"id":"3dmslK1N-kS0"},"source":["### 3.1 Downloading the dataset\n","First of all we need to download our dataset. You can directly read the data from this link: `https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv` into a pandas `DataFrame`."]},{"cell_type":"code","metadata":{"id":"LON-cg0W-p0y"},"source":["# create a pandas dataframe and save it in a variable called data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rw2nNB84-vee"},"source":["Explore the data.\n","* What data kind of data is it?\n","* How large is the dataset?\n","* What are the labels?\n","* How are the labels distributed?"]},{"cell_type":"code","metadata":{"id":"v9cXEAoa-wDL"},"source":["# get the first 10 rows of the data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YERljPCV-0pF"},"source":["# how many training examples do we have?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRD4jprO-3S_"},"source":["# how many labels differnt do we have?\n","# how many labels do we have for each class?\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j8FcZ4mUt-fY"},"source":["### 3.2 Preprocessing the corpus\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H99h6WetDLId"},"source":["Implement a function that gets a text as an input and returns the preprocessed version of it. Your preprocessing should include the following steps:\n","* lowercase text\n","* remove punctuation\n","* remove stopwords\n","* remove numbers\n","* stem text\n","\n","you can use the gensim library for the preprocessing. Visit the [documentation](https://radimrehurek.com/gensim_3.8.3/parsing/preprocessing.html) to learn how it works.\n"]},{"cell_type":"code","metadata":{"id":"eg8x5XvUx_rS"},"source":["# import gensim library for preprocessing\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mB00uMwyBTj"},"source":["# define preprocessing function\n","def preprocess_text(text):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADqnGtRfF0AK"},"source":["Execute the following Cell. Your function should print out something like ```text includes number```."]},{"cell_type":"code","metadata":{"id":"Or8tl9koFltU"},"source":["test_text = \"This is a text that INCLUDES the number 34.\"\n","preprocess_text(test_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4A6ZtJYe_d1l"},"source":["# apply it to the text in our DataFrame\n","# and save the results in a new column called review_pp\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gy0TH1JKVJ-Q"},"source":["Some words happen to appear very frequently next to each other. It can be very useful to combine this word into a single representation before tokenizing the corpus.\n","\n","The combination of two words is called bigram. Have a look at the following examples:\n","* new york -> new_york\n","* star wars -> star_wars\n","\n","The following code blocks show how you can train a phraser with the gensim library. You can learn more about the Phraser [here](https://radimrehurek.com/gensim_3.8.3/models/phrases.html).\n","\n"]},{"cell_type":"code","metadata":{"id":"5ImDi2hIVvEP"},"source":["from gensim.models.phrases import Phrases\n","phrases = Phrases(data['review_pp'], threshold=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af4oUpOsfx0A"},"source":["You can apply the bigram model like this:"]},{"cell_type":"code","metadata":{"id":"tMt6qqtOsbBY"},"source":["phrases[\"the lion king is by far my favorite movie!\".split()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Emo0VtWehhW"},"source":["Apply the bigrams to the text in the `review_pp` column and overwrite it."]},{"cell_type":"code","metadata":{"id":"CMY_NNaDVH3I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlP9tffDfDul"},"source":["### 3.3 Creating a Corpus Class"]},{"cell_type":"markdown","metadata":{"id":"n4tUo8DpdZvo"},"source":["In this section we create a class for our corpus that will be the input for the training algorithm of our word2vec model.\n","\n","Write a class called `MyCorpus`.\n","It should have an `__init__` function that takes a list or array of strings as an input and saves it into a class variable `self.data`.\n","\n","Also implement an `__iter__` function that loops over `self.data` and returns each line as a list of the words. (hint: use `.split()`to split a string into words"]},{"cell_type":"code","metadata":{"id":"_8rVmG3rc9Us"},"source":["class MyCorpus:\n","  def __init__(self, data):\n","    pass\n","\n","  def __iter__(self,):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KfJSpNvthXw"},"source":["# instantiate the MyCorpus class with a list of the preprocessed texts in our dataframe.\n","# save it in a variable called sentences\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eqvn9TUWvdl"},"source":["%%time\n","model = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heGORJoSgIMJ"},"source":["Once again explore the model by using the similarity function. Remember that this is a very specific dataset we trained the model on. It is about movie reviews and the analogies we used before probably won't work here.\n","\n","But maybe there are some interesting relationships between actors and movies encoded in the word embeddings."]},{"cell_type":"code","metadata":{"id":"CzR8J-H_W-KM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPzlJLkKyjsN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42EKljcXiWrD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ka4sJXubiZJc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PCHSrLQicfH"},"source":["word_list = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrN7sKh24iWk"},"source":["## Visualizing Word Embeddings\n","Once again create a list that you want to plot in 2D-Space.\n","Try to use a list of movie titles and actors and see how similar they are."]},{"cell_type":"code","metadata":{"id":"Tt3kMQDI4k5J"},"source":["display_pca_scatterplot(model, word_list)"],"execution_count":null,"outputs":[]}]}